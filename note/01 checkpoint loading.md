## Checkpoint loading



í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ë•Œ í•™ìŠµ í™˜ê²½ì´ ë‹¤ë¥¸ ê²½ìš° statedict loading ê³¼ì •ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤. íŠ¹íˆ `AutoModel.from_pretrained()` ì— ìµìˆ™í•œ ê²½ìš° torchë¡œ í•™ìŠµí•œ ëª¨ë¸ì„ ê°€ì ¸ì˜¬ ë•Œ ì˜ˆìƒí•˜ì§€ ëª»í•œ ì—ëŸ¬ë©”ì„¸ì§€ë¥¼ ë³´ê³  ë‹¹í™©í•˜ê¸° ì‰½ê¸°ì— ê´€ë ¨ë‚´ìš©ì„ ì •ë¦¬í•´ë³´ê³ ì í•œë‹¤.



> íŠ¹íˆ ê´€ë ¨ ë‚´ìš©ì€ ë¶€ìŠ¤íŠ¸ìº í”„ ë©˜í† ë§ì„ ì§„í–‰í•˜ë©° ìµœì†Œ 5ë²ˆ ì´ìƒ ë°›ì€ ì§ˆë¬¸ì´ë©°, ë‚˜ ë˜í•œ ëª¨ë¸ë§ì„ ì§„í–‰í•˜ë©° ë™ì¼í•œ ì—ëŸ¬ë¥¼ ë§Œë‚œì  ìˆë‹¤.



#### 01 Model load

ê¸°ë³¸ì ìœ¼ë¡œ  `AutoModel.from_pretrained()` ì€ hugging face model hubì—ì„œ ì œê³µë˜ëŠ” ëª¨ë¸ ì´ë¦„ì„ ë„£ì–´ pretrained modelì„ Loading í•˜ëŠ”ê²ƒì´ ì¼ë°˜ì ì´ë‹¤. í˜¹ì€ ë‚´ê°€ ìƒì„±í•œ ëª¨ë¸ì´ ì €ì¥ëœ pathë¥¼ ë„£ì–´ ì €ì¥ëœ checkpointë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ë„ ìˆë‹¤. 

ê°„í˜¹ finetuned modelì˜ ì´ë¦„ì„ hubì—ì„œ ì œê³µí•˜ëŠ” ì´ë¦„ê³¼ ë™ì¼í•˜ê²Œ ì„¸íŒ…í•˜ì—¬ ì €ì¥í•˜ëŠ” ê²½ìš°ê°€ ìˆëŠ”ë°, custom nameì„ ì£¼ëŠ”ê²ƒì´ ì¢‹ë‹¤. í˜¹ì€ `local_files_only` optionì„ `True`ë¡œ ì„¤ì •í•˜ì—¬ ì—ëŸ¬ë¥¼ ë°©ì§€í•  ìˆ˜ë„ ìˆë‹¤. 

```python
from transformers import AutoModel
AutoModel.from_pretrained(MODEL_PATH, local_files_only=True)
```



#### 02 Loadingì‹œ ì²´í¬í•´ë³´ë©´ ì¢‹ì„ ì‚¬í•­

ê°„í˜¹ í•™ìŠµí™˜ê²½ì´ ìƒì´í•œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ loadingì„ ì§„í–‰í•˜ëŠ” ê²½ìš°ê°€ ìˆëŠ”ë°, ì´ë•Œ ëª‡ê°€ì§€ ì—ëŸ¬ë¥¼ ë§Œë‚  ìˆ˜ ìˆë‹¤

##### Case 1 : load modelì˜ state dictì´ configì™€ ë‹¤ë¥¸ ê²½ìš°

* ê°€ì¥ í”í•˜ê²Œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ê²½ìš°ë¡œ, í•™ìŠµ ëª¨ë¸ì´ ddpë¡œ í•™ìŠµë˜ì—ˆê±°ë‚˜ ë²„ì „ì´ ë‹¤ë¥¼ ê²½ìš° model state dict nameì´ ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤.

* ì´ ê²½ìš°ì—ëŠ” ì•„ë˜ì™€ ê°™ì€ ë©”ì„¸ì§€ê°€ ëœ¨ë©° keyê°€ matchë˜ì§€ì•Šì•„ ëª¨ë¸ì„ ë¡œë“œ í•  ìˆ˜ ì—†ë‹¤ê³  ë‚˜ì˜¨ë‹¤

  ~~~
  _IncompatibleKeys(missing_keys=[..],)
  ~~~

* ì˜ˆë¥¼ë“¤ì–´ bert.encoder.embeddings ~ vs encoder.embeddings ~ ì€ êµ¬ì¡°ê°€ ë™ì¼í•˜ì§€ë§Œ Key ê°’ì´ ë§¤ì¹˜ë˜ì§€ ì•ŠëŠ” ìƒí™©ì´ë¼ê³  ì´í•´í•  ìˆ˜ ìˆë‹¤
* êµ¬ì¡°ê°€ ë™ì¼í•œ ê²½ìš° key ê°’ì„ ë§ì¶°ì£¼ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë©°, ëŒ€ë¶€ë¶„ ì•„ë˜ì™€ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ìˆ˜ì •í•˜ë©´ ëœë‹¤
* í•´ë‹¹ì˜ˆì‹œëŠ” BERT ëª¨ë¸ì„ ê¸°ì¤€ìœ¼ë¡œ í•œë‹¤

~~~python
from transformers import AutoModel,AutoConfig
import glob
import torch
from collections import OrderedDict

model = AutoModel.from_pretrained(Model_name)
state_dict = torch.load(Model_path, map_location='cpu')

keys = list(state_dict.keys())
for key in keys:
    if 'LayerNorm' in key:
        if 'gamma' in key:
            state_dict[key.replace('gamma', 'weight')] = state_dict.pop(key)
        else:
            state_dict[key.replace('beta', 'bias')] = state_dict.pop(key)

new_state_dict = OrderedDict()

for k, v in state_dict.items():
    name = k[5:] if k[:5] == 'bert.' else k # remove `bert.`
    new_state_dict[name] = v

model.load_state_dict(new_state_dict, strict=False)
~~~

* ì—ëŸ¬ ë©”ì„¸ì§€ë¥¼ í™•ì¸í•´ë³´ë©´ ì–´ë–¤ Key ê°’ì—ì„œ ë¬¸ì œê°€ ìƒê¸´ì§€ ëª…ì‹œí•´ì£¼ê¸° ë•Œë¬¸ì— í•˜ë‚˜ì”© í‚¤ê°’ì„ ìˆ˜ì •í•˜ëŠ”ê²Œ ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì´ë‹¤
* `strict=False`  ì„ ì‚¬ìš©í•˜ë©´ ë§¤ì¹˜ë˜ëŠ” í‚¤ê°’ë§Œ weightë¥¼ êµì²´í•´ì£¼ê¸° ë•Œë¬¸ì— ë¹„êµì  ìœ ì—°í•˜ê²Œ ê°’ì„ ë„£ì–´ì¤„ ìˆ˜ ìˆìœ¼ë©°, ë˜ë„ë¡ ì‚¬ìš©í•˜ì—¬ ì–´ë–¤ ë¬¸ì œê°€ ìˆëŠ”ì§€ ì²´í¬í•˜ëŠ”ê²ƒì´ ì¢‹ë‹¤.
  * ì´ê±¸ ì“°ì§€ ì•Šìœ¼ë©´ í•˜ë‚˜ì”© ì²´í¬í•˜ë©´ì„œ weightê°€ ì˜ ë“¤ì–´ê°”ëŠ”ì§€ í™•ì¸í•´ì¤˜ì•¼í•œë‹¤
  * í˜¹ì€ ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•ŠëŠ” ëŒ€ì‹  ì´ˆê¸°í™”ëœ ê°€ì¤‘ì¹˜ê°€ ë“¤ì–´ê°€ ì˜ë„í•˜ì§€ ì•Šì€ ë°©í–¥ìœ¼ë¡œ inferenceë¥¼ ì§„í–‰í•˜ëŠ” ëŒ€ì°¸ì‚¬ê°€ ì¼ì–´ë‚  ìˆ˜ ìˆë‹¤
    * ëŒ€ì°¸ì‚¬ê°€ ì¼ì–´ë‚¬ë˜ ê²½í—˜ì´ ìˆë‹¤ ğŸ¤“ (ã…‹ã…‹)



##### Case 2 loadí•œ ëª¨ë¸ì´ ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” ëª¨ë¸ê³¼ êµ¬ì¡°ê°€ ë‹¤ë¥¸ ê²½ìš°

>  Some weights of the model checkpoint at 'MODEL_NAME' were not used when initializing BertModel

ìµœê·¼ TAPT, DAPTë¥¼ ìˆ˜í–‰í•œ ë‹¤ìŒì— ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµí•˜ëŠ” ê²½ìš°ê°€ ë§ì€ë°, ìì£¼ ë§Œë‚˜ë³¼ ìˆ˜ ìˆëŠ” ë©”ì„¸ì§€ì´ë‹¤. ì´ê±´ í¬ê²Œ ë¬¸ì œê°€ ë˜ì§€ ì•ŠëŠ” ë©”ì„¸ì§€ì´ì§€ë§Œ, ê´œíˆ í„°ë¯¸ë„ì— ë‹¤ìŒê³¼ ê°™ì€ ë©”ì„¸ì§€ê°€ ëœ¨ë©´ ì‹ ê²½ ì“°ì¼ ìˆ˜ ë°–ì— ì—†ë‹¤.

ë©”ì„¸ì§€ë¥¼ ì˜ ì½ì–´ë³´ë©´ í° ë¬¸ì œê°€ ì•„ë‹˜ì„ ì•Œ ìˆ˜ ìˆë‹¤. BertForPretrainingìœ¼ë¡œ í•™ìŠµí•˜ê³  ë‚˜ì„œ downstream taskë¥¼ ìˆ˜í–‰í•˜ë©´ key matchê°€ ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì´ì•¼ê¸°ì´ê³ , ì–´ë–¤ ìƒí™©ì—ì„œëŠ” ê°€ëŠ¥í•˜ê³  ì–´ë–¤ìƒí™©ì—ì„œëŠ” ë¬¸ì œê°€ ë˜ëŠ”ì§€ ëª…í™•íˆ ë§í•´ì£¼ê³  ìˆë‹¤.

> * This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). 
>
> - This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

(transformersëŠ” ì—ëŸ¬ ë©”ì„¸ì§€ê°€ ì¹œì ˆí•˜ë‹¤)